<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to retrieve packet drop reasons in the Linux kernel</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/19/how-retrieve-packet-drop-reasons-linux-kernel" /><author><name>Antoine Tenart</name></author><id>c81250ec-0fca-472f-9d40-943ba5153745</id><updated>2023-07-19T07:00:00Z</updated><published>2023-07-19T07:00:00Z</published><summary type="html">&lt;p&gt;Understanding why a packet drops in the Linux kernel is not always easy. The networking stack is wide and reasons to refuse a given packet are multiple and include invalid data from a protocol, firewall rules, wrong checksum, full queues, qdisc or XDP actions, and many more reasons. It is possible to look at indicators such as MIB counters and statistic counters, but often those are generic and triggered for different reasons, but most importantly their coverage is small, and it's impossible to match a specific packet to a given counter increase. &lt;/p&gt; &lt;h2&gt;Socket buffer drop reasons&lt;/h2&gt; &lt;p&gt;The socket buffer, SKB (&lt;code&gt;struct sk_buff&lt;/code&gt;) is the main data structure representing a packet in the Linux kernel networking stack. When a packet is dropped in the Linux kernel, in most cases, it means its associated socket buffer has dropped. In recent versions of the Linux kernel, starting in v5.17, socket buffers can be dropped with an associated reason. This was introduced in upstream commit &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=c504e5c2f9648a1e5c2be01e8c3f59d394192bd3"&gt;c504e5c2f964 ("net: skb: introduce kfree_skb_reason()")&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Using this commit and later additions, kernel developers are now able to specify why a given packet dropped. In the following example, a packet is dropped because no socket was found:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;- kfree_skb(skb); + kfree_skb_reason(skb, SKB_DROP_REASON_NO_SOCKET);&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using tools to retrieve drop reasons&lt;/h2&gt; &lt;p&gt;The SKB drop reason can be retrieved in a few different ways, depending on which you are comfortable using, what is available on a given system, and the end goal (some solutions have more flexibility than others).&lt;/p&gt; &lt;p&gt;The main interface to retrieve the drop reason is the &lt;code&gt;skb:kfree_skb&lt;/code&gt; tracepoint. It provides a user readable text for all drop reasons. A good way to attach to this tracepoint is to use &lt;code&gt;perf&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# perf record -e skb:kfree_skb curl https://localhost # given no server listens on localhost:443/tcp. # perf script curl 883 [001] 340.799805: skb:kfree_skb: skbaddr=0xffff88811f6a7068 protocol=2048 location=tcp_v4_rcv+0x157 reason: NO_SOCKET curl 883 [001] 340.800860: skb:kfree_skb: skbaddr=0xffff88811f6a6de8 protocol=34525 location=tcp_v6_rcv+0x137 reason: NO_SOCKET&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can see why the two packets where dropped in &lt;code&gt;tcp_v4_rcv&lt;/code&gt; and &lt;code&gt;tcp_v6_rcv&lt;/code&gt; because no socket was found and we do not have a server listening on &lt;code&gt;localhost:443/tcp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We can also use other tools such as &lt;code&gt;bpftrace&lt;/code&gt; to get the drop reason, which would give us more flexibility, the drawback being the reason isn't converted to a human readable string:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# bpftrace -e 'tracepoint:skb:kfree_skb {printf("%s: %d\n", comm, args-&gt;reason)}' -c 'curl https://localhost' Attaching 1 probe... curl: 3 curl: 3 curl: (7) Failed to connect to localhost port 443 after 2 ms: Couldn't connect to server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another method is the &lt;code&gt;dropwatch&lt;/code&gt;, an interactive tool to monitor packets dropped in the Linux kernel. When using the &lt;code&gt;packet alert mode&lt;/code&gt;, drop reasons are included.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# dropwatch -l kas Initializing kallsyms db dropwatch&gt; set alertmode packet Setting alert mode Alert mode successfully set dropwatch&gt; start Enabling monitoring... Kernel monitoring activated. Issue Ctrl-C to stop monitoring drop at: tcp_v4_rcv+0x157/0x1630 (0xffffffff8abc4f87) origin: software input port ifindex: 1 timestamp: Thu Feb 23 18:03:36 2023 370138884 nsec protocol: 0x800 length: 74 original length: 74 drop reason: NO_SOCKET drop at: tcp_v6_rcv+0x137/0x14f0 (0xffffffff8ad91b37) origin: software input port ifindex: 1 timestamp: Thu Feb 23 18:03:36 2023 372335338 nsec protocol: 0x86dd length: 94 original length: 94 drop reason: NO_SOCKET&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;The &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/net/dropreason-core.h?h=v6.4-rc1#n88"&gt;&lt;code&gt;skb_drop_reason enum&lt;/code&gt;&lt;/a&gt; defines core drop reasons. It is an internal definition, and the actual value of all its members is not guaranteed to be constant over time. This feature is recent and some of the drop reasons were reordered during development. There is also work ongoing for supporting drop reasons from different subsystems. You should either use tools directly providing the drop reason in a text format (perf or dropwatch) or take the right drop reasons definition as a reference when retrieving the drop reason in a numeric way (bpftrace).&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Not all drop places in the Linux kernel are covered. Converting them to this new facility takes time and resources. There is progress upstream with more additions. Currently, more than &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/net/dropreason.h?h=v6.2#n81"&gt;70 reasons are supported&lt;/a&gt;. There is also an effort to support more than the core networking subsystem.&lt;/p&gt; &lt;p&gt;SKB drop reasons are now available in &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; starting with RHEL 8.8 and RHEL 9.2.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/19/how-retrieve-packet-drop-reasons-linux-kernel" title="How to retrieve packet drop reasons in the Linux kernel"&gt;How to retrieve packet drop reasons in the Linux kernel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Antoine Tenart</dc:creator><dc:date>2023-07-19T07:00:00Z</dc:date></entry><entry><title type="html">Top 20 Must-Read Software Reports</title><link rel="alternate" href="http://www.ofbizian.com/2023/07/top-20-must-read-software-reports.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2023/07/top-20-must-read-software-reports.html</id><updated>2023-07-18T23:23:00Z</updated><content type="html">In the rapidly evolving software industry, keeping up with trends, tools, and best practices is not easy. But with so much information available, where do you start? I've curated a list of reports that I follow to stay informed and ahead of the curve. These provide insights into everything from programming languages to DevOps, cloud strategy, and security. Here are the top 20 reports I keep an eye on. ReportPublisherTIOBERedMonkStack OverflowInfoQInfoQPostmanDataDogThoughtworksO'ReillyHashiCorpRed HatPuppetLabsVmWareDenoAirByteDatabricksGithubRapidAPICNCFGoogle While these reports offer valuable insights, it's important to keep in mind that they can be opinionated. The key to effectively leveraging these resources lies in cross-verifying trends from multiple sources and using them only as a guide for direction rather than absolute truths. You can catch some of my insights and explorations on various topics over at the . Did I miss any insightful reports? Feel free to suggest them in the following . I'm always keen to explore new sources! Found this list helpful? Go ahead,</content><dc:creator>Unknown</dc:creator></entry><entry><title>How to run a custom server task in Red Hat Data Grid</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/18/how-run-custom-server-task-red-hat-data-grid" /><author><name>Torbjorn Dahlen</name></author><id>d88d032d-2289-48ea-a518-eea26fcb73be</id><updated>2023-07-18T07:00:00Z</updated><published>2023-07-18T07:00:00Z</published><summary type="html">&lt;p&gt;Custom server tasks can be deployed to the &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid"&gt;Red Hat Data Grid&lt;/a&gt; server for remote execution from the command line interface (CLI) and Hot Rod or REST clients. Tasks can be implemented as custom Java classes or as scripts in languages such as JavaScript.&lt;/p&gt; &lt;p&gt;In this article, we will deploy a Java class that will evict and reload the cache in order to pick up modified entries in the original database table from which we loaded the cache. Data Grid will automatically load new entries added to the database table, however modified rows will require reloading using a server task, as shown in the following example.&lt;/p&gt; &lt;h2&gt;How to deploy PostgreSQL with a custom image&lt;/h2&gt; &lt;p&gt;SQL cache stores let you load Red Hat Data Grid caches from existing database tables. Data Grid offers two types of SQL cache stores: table and query. In the following example, Data Grid will load entries from a single database table. It is also possible to use SQL queries to load entries from single or multiple database tables.&lt;/p&gt; &lt;p&gt;For more details, refer to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/configuring_data_grid_caches/persistence#sql-cache-store_persistence"&gt;SQL cache stores&lt;/a&gt;. All source code for this tutorial can be found on &lt;a href="https://github.com/torbjorndahlen/infinispan-evict-cache"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We will use PostgreSQL to contain the table that will be loaded by Data Grid. To deploy PostgreSQL in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, we will use a modified image that initializes the database with a table and a Data Grid user with granted privileges to read from the table.&lt;/p&gt; &lt;pre&gt; FROM registry.redhat.io/rhel8/postgresql-12 LABEL description="This is a custom PostgreSQL container image which loads the database schema definitions and the data into the model and inventory tables " COPY db/load_db.sh /opt/app-root/src/postgresql-start/ COPY db/rpi-store-ddl.sql /opt/app-root/src/postgresql-start/ COPY db/rpi-store-dml.sql /opt/app-root/src/postgresql-start/ COPY db/rpi-store-role.sql /opt/app-root/src/postgresql-start/ USER root RUN chmod 774 /opt/app-root/src/postgresql-start/*.sh USER 26&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;load_db.sh&lt;/code&gt; script populates the table used to load the Data Grid cache and creates the user &lt;code&gt;infinispan&lt;/code&gt; with privileges to &lt;code&gt;SELECT&lt;/code&gt; from the &lt;code&gt;model&lt;/code&gt; table:&lt;/p&gt; &lt;pre&gt; #!/bin/bash START_DIR="$APP_DATA/src/postgresql-start" run_sql_script () { SQL_FILE=$1 psql -U postgres \ --echo-all \ -f $SQL_FILE \ -d $POSTGRESQL_DATABASE } run_sql_script $START_DIR/rpi-store-ddl.sql run_sql_script $START_DIR/rpi-store-dml.sql run_sql_script $START_DIR/rpi-store-role.sql&lt;/pre&gt; &lt;p&gt;Note that the user needs to be &lt;code&gt;postgres&lt;/code&gt; to create a new user.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;rpi-store-ddl.sql&lt;/code&gt; script will run the following SQL commands to create the &lt;code&gt;model&lt;/code&gt; table which will be used to load keys and values into the Data Grid cache:&lt;/p&gt; &lt;pre&gt; drop table if exists model; create table model ( id integer primary key, name varchar(20), model varchar(20), soc varchar(20), memory_mb integer, ethernet boolean, release_year integer );&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;rpi-store-dml.sql&lt;/code&gt;script will create three rows in the &lt;code&gt;model&lt;/code&gt; table:&lt;/p&gt; &lt;pre&gt; insert into model (id, name, model, soc, memory_mb, ethernet, release_year) values (1, 'Raspberry Pi', 'B', 'BCM2835', 256, TRUE, 2012); insert into model (id, name, model, soc, memory_mb, ethernet, release_year) values (2, 'Raspberry Pi Zero', 'Zero', 'BCM2835', 512, FALSE, 2015); insert into model (id, name, model, soc, memory_mb, ethernet, release_year) values (3, 'Raspberry Pi Zero', '2W', 'BCM2835', 512, FALSE, 2021);&lt;/pre&gt; &lt;p&gt;Finally, the &lt;code&gt;rpi-store-role.sql&lt;/code&gt; script will create the user &lt;code&gt;infinispan&lt;/code&gt; and grant &lt;code&gt;SELECT&lt;/code&gt; privileges on the &lt;code&gt;model&lt;/code&gt; table. This user will be provided to the JDBC connector used by Data Grid.&lt;/p&gt; &lt;pre&gt; CREATE USER infinispan WITH PASSWORD 'secret'; GRANT SELECT ON model TO infinispan;&lt;/pre&gt; &lt;p&gt;We can now deploy PostgreSQL in OpenShift using the modified image:&lt;/p&gt; &lt;pre&gt; $ oc new-project infinispan-demo $ oc new-build \ &gt; https://github.com/torbjorndahlen/infinispan-evict-cache \ &gt; --strategy=docker \ &gt; --name='postgresql-12-custom' $ oc new-app \ &gt; -e POSTGRESQL_USER=db \ &gt; -e POSTGRESQL_PASSWORD=secret \ &gt; -e POSTGRESQL_DATABASE=rpi-store \ &gt; postgresql-12-custom&lt;/pre&gt; &lt;p&gt;After deployment is complete, we can verify that the user &lt;code&gt;infinispan&lt;/code&gt;, the DB &lt;code&gt;rpi-store&lt;/code&gt;, and the &lt;code&gt;model&lt;/code&gt; table were created as expected:&lt;/p&gt; &lt;pre&gt; $ oc get pods NAME READY STATUS RESTARTS AGE postgresql-12-custom-1-build 0/1 Completed 0 2m40s postgresql-12-custom-0 1/1 Running 0 38s $ oc exec postgresql-12-custom-0 -- psql -U infinispan -d rpi-store -c "select * from model;" id | name | model | soc | memory_mb | ethernet | release_year ----+-------------------+-------+---------+-----------+----------+-------------- 1 | Raspberry Pi | B | BCM2835 | 256 | t | 2012 2 | Raspberry Pi Zero | Zero | BCM2835 | 512 | f | 2015 3 | Raspberry Pi Zero | 2W | BCM2835 | 512 | f | 2021 (3 rows)&lt;/pre&gt; &lt;h2&gt;Server task implementation&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;EvictReloadTask&lt;/code&gt; class implements the &lt;code&gt;org.infinispan.tasks.ServerTask&lt;/code&gt; interface where the &lt;code&gt;call()&lt;/code&gt; method is invoked by Data Grid when called from the Hot Rod client:&lt;/p&gt; &lt;pre&gt; @MetaInfServices(ServerTask.class) public class EvictReloadTask implements ServerTask, java.io.Serializable { private static final ThreadLocal taskContext = new ThreadLocal&lt;&gt;(); @Override public String call() throws Exception { TaskContext ctx = taskContext.get(); AdvancedCache&lt;?, ?&gt; cache = ctx.getCacheManager().getCache("rpi-store").getAdvancedCache(); cache.withFlags(Flag.SKIP_CACHE_STORE).clear(); cache.getComponentRegistry().getComponent(PreloadManager.class).start(); return null; } }&lt;/pre&gt; &lt;p&gt;Before deploying Data Grid, the server task is packaged in a JAR file containing the server task classes and a file in the &lt;code&gt;META-INF/services&lt;/code&gt; directory. This file is named &lt;code&gt;org.infinispan.tasks.ServerTask&lt;/code&gt; and contains the fully qualified name of the server task.&lt;/p&gt; &lt;pre&gt; example.EvictReloadTask&lt;/pre&gt; &lt;p&gt;You also need to add your server task classes to a deserialization allow list, since Data Grid does not allow deserialization of arbitrary Java classes for security reasons. To do this, we create a ConfigMap containing an allow-list for serializing of the server task class:&lt;/p&gt; &lt;pre&gt; apiVersion: v1 kind: ConfigMap metadata: name: cluster-config namespace: infinispan-demo data: infinispan-config.xml: &gt; &lt;infinispan&gt; &lt;cache-container&gt; &lt;serialization marshaller="org.infinispan.commons.marshall.JavaSerializationMarshaller"&gt; &lt;allow-list&gt; &lt;class&gt;example.EvictReloadTask&lt;/class&gt; &lt;/allow-list&gt; &lt;/serialization&gt; &lt;/cache-container&gt; &lt;/infinispan&gt;&lt;/pre&gt; &lt;p&gt;Then, we deploy the ConfigMap:&lt;/p&gt; &lt;pre&gt; $ oc apply -f cluster-config.yaml&lt;/pre&gt; &lt;h2&gt;Deploy Data Grid&lt;/h2&gt; &lt;p&gt;To deploy the Data Grid cluster in OpenShift we use the Data Grid operator.&lt;/p&gt; &lt;p&gt;Install the operator:&lt;/p&gt; &lt;pre&gt; $ oc apply -f infinispan-operator.yaml $ oc apply -f subscription.yaml&lt;/pre&gt; &lt;p&gt;In this example, we will use the following custom resource to let the operator create a Data Grid cluster:&lt;/p&gt; &lt;pre&gt; apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: infinispan namespace: infinispan-demo spec: security: endpointEncryption: type: None clientCert: None expose: type: LoadBalancer dependencies: artifacts: - maven: 'org.postgresql:postgresql:42.3.1' - url: &gt;- https://github.com/torbjorndahlen/infinispan-evict-cache/raw/main/ServerTask/server/target/ServerTask.jar service: type: DataGrid replicas: 1 configMapName: cluster-config&lt;/pre&gt; &lt;p&gt;The Maven artifact refers to the JDBC driver for PostgreSQL. The URL artifact refers to the Git repository where the server task JAR file can be downloaded.&lt;/p&gt; &lt;p&gt;The Data Grid cluster is created with oc create:&lt;/p&gt; &lt;pre&gt; $ oc create -f infinispan-cr.yaml&lt;/pre&gt; &lt;p&gt;Use &lt;code&gt;oc get svc&lt;/code&gt; to find the URL to the Data Grid console:&lt;/p&gt; &lt;pre&gt; $ oc get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE example-infinispan-external LoadBalancer 172.30.5.74 my_host.example.com 11222:31797/TCP 6m39s&lt;/pre&gt; &lt;p&gt;In this example, we exposed Data Grid through a loadbalancer. The URL to the Data Grid contains the loadbalancer hostname and port. The Data Grid console can be accessed at &lt;code&gt;http://my_host.example.com:11222&lt;/code&gt;. The console username and password is stored in the &lt;code&gt;infinispan-generated-secret&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; $ oc get secret infinispan-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode credentials: - username: developer password: my_password roles: - admin&lt;/pre&gt; &lt;h2&gt;Create the cache&lt;/h2&gt; &lt;p&gt;You can use SQL stores with database tables that contain composite primary keys or composite values.&lt;/p&gt; &lt;p&gt;To use composite keys or values, you must provide Data Grid with protobuf schema that describe the data types. You must also add schema configuration to your SQL store and specify the message names for keys and values.&lt;/p&gt; &lt;p&gt;You can find more information on &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/cache_encoding_and_marshalling/index#doc-wrapper"&gt;cache encoding and marshaling&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Using the Data Grid console, create a cache using a protobuf schema and SQL cache store configuration, as shown in Figure 1:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/skarmavbild_2023-04-20_kl._12.38.50.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/skarmavbild_2023-04-20_kl._12.38.50.png?itok=Xd9U3r3w" width="600" height="452" alt="A screenshot of the Data Grid console, creating a cache using a protobuf schema." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Creating a cache using a protobuf schema in the Data Grid console.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Use the following configuration for the cache. In the configuration, &lt;code&gt;table-name&lt;/code&gt; refers to the &lt;code&gt;model&lt;/code&gt; table created when PostgreSQL was deployed. The &lt;code&gt;message-name&lt;/code&gt; and &lt;code&gt;package&lt;/code&gt; refers to the protobuf schema. Notice the user &lt;code&gt;infinispan&lt;/code&gt; that was previously created is used by the JDBC driver.&lt;/p&gt; &lt;pre&gt; { "distributed-cache": { "mode": "SYNC", "encoding": { "key": { "media-type": "application/x-protostream" }, "value": { "media-type": "application/x-protostream" } }, "persistence": { "table-jdbc-store": { "shared": true, "segmented": false, "dialect": "POSTGRES", "table-name": "model", "schema": { "message-name": "model_value", "package": "example" }, "connection-pool": { "connection-url": "jdbc:postgresql://postgresql-12-custom:5432/rpi-store", "driver": "org.postgresql.Driver", "username": "infinispan", "password": "secret" } } } } }&lt;/pre&gt; &lt;p&gt;When created, the cache will automatically load the &lt;code&gt;model&lt;/code&gt; table, as shown in Figure 2:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/skarmavbild_2023-04-20_kl._12.52.01.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/skarmavbild_2023-04-20_kl._12.52.01.png?itok=0NTHk2Jh" width="600" height="489" alt="Shows the cache with entries from the model table." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The cache is loaded with entries from the model table.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Run the server task&lt;/h2&gt; &lt;p&gt;In our example, the table from which Data Grid loaded the entries to the cache will be modified without notifying Data Grid. To update the cache with the modified entry, the server task will be called from a Hot Rod client.&lt;/p&gt; &lt;p&gt;First, we update a row in the model table:&lt;/p&gt; &lt;pre&gt; $ oc exec postgresql-12-custom-0 -- psql -U postgres -d rpi-store -c "update model set name = 'Raspberry Pi UPDATED' where id = 1;" $ oc exec postgresql-12-custom-0 -- psql -U infinispan -d rpi-store -c "select * from model;" id | name | model | soc | memory_mb | ethernet | release_year ----+-----------------------+-------+---------+-----------+----------+-------------- 2 | Raspberry Pi Zero | Zero | BCM2835 | 512 | f | 2015 3 | Raspberry Pi Zero | 2W | BCM2835 | 512 | f | 2021 1 | Raspberry Pi UPDATED | B | BCM2835 | 256 | t | 2012 (3 rows)&lt;/pre&gt; &lt;p&gt;Verify that the cache doesn't contain the updated entry by using the Infinispan CLI to lookup the key 1 in the cache:&lt;/p&gt; &lt;pre&gt; $ oc get pods NAME READY STATUS RESTARTS AGE infinispan-0 1/1 Running 0 86s $ oc rsh infinispan-0 sh-4.4$./bin/cli.sh [disconnected]&gt; connect Username: developer Password: my_password [infinispan-0-28040@infinispan//containers/default]&gt; cd caches [infinispan-0-28040@infinispan//containers/default/caches]&gt; cd rpi-store [infinispan-0-28040@infinispan//containers/default/caches/rpi-store]&gt; get 1 { "_type" : "example.model_value", "name" : "Raspberry Pi", "model" : "B", "soc" : "BCM2835", "memory_mb" : 256, "ethernet" : true, "release_year" : 2012 }&lt;/pre&gt; &lt;p&gt;The cache hasn't been notified about the modified row and still contains the name, &lt;strong&gt;Raspberry Pi&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;To load the modified entry into the cache, we run the client. The username and password are located under &lt;strong&gt;Secrets&lt;/strong&gt; in &lt;code&gt;infinispan-generated-secret&lt;/code&gt;.&lt;strong&gt; &lt;/strong&gt;The Data Grid server can be accessed from the infinispan loadbalancer hostname and port.&lt;/p&gt; &lt;pre&gt; $ git clone https://github.com/torbjorndahlen/infinispan-evict-cache.git $ cd infinispan-evict-cache/ServerTask/client $ mvn clean package $ mvn assembly:assembly -DdescriptorId=jar-with-dependencies $ java -cp target/ServerTaskClient-jar-with-dependencies.jar \ &gt; example.CacheServerTaskInvocation \ &gt; my_host.example.com 11222 \ &gt; developer my_password rpi-store&lt;/pre&gt; &lt;p&gt;Verify that the cache now contains the modified entries by using the Infinispan CLI to lookup the key 1 in the cache:&lt;/p&gt; &lt;pre&gt; $ oc rsh infinispan-0 sh-4.4$./bin/cli.sh [disconnected]&gt; connect Username: developer Password: my_password [infinispan-0-28040@infinispan//containers/default]&gt; cd caches [infinispan-0-28040@infinispan//containers/default/caches]&gt; cd rpi-store [infinispan-0-28040@infinispan//containers/default/caches/rpi-store]&gt; get 1 { "_type" : "example.model_value", "name" : "Raspberry Pi UPDATED", "model" : "B", "soc" : "BCM2835", "memory_mb" : 256, "ethernet" : true, "release_year" : 2012 }&lt;/pre&gt; &lt;p&gt;The updated value for &lt;strong&gt;key 1&lt;/strong&gt; has been loaded into the cache.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated how a cache using a PostgreSQL database as a cache store can be refreshed by using a server task invoked from a Hot Rod client when a table is being updated by other means than passing through Data Grid.&lt;/p&gt; &lt;p&gt;Thanks to Tristan Tarrant at Red Hat for providing advice and suggestions on the implementation of this tutorial.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/18/how-run-custom-server-task-red-hat-data-grid" title="How to run a custom server task in Red Hat Data Grid"&gt;How to run a custom server task in Red Hat Data Grid&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Torbjorn Dahlen</dc:creator><dc:date>2023-07-18T07:00:00Z</dc:date></entry><entry><title type="html">Keycloak 22.0.1 released</title><link rel="alternate" href="https://www.keycloak.org/2023/07/keycloak-2201-released" /><author><name /></author><id>https://www.keycloak.org/2023/07/keycloak-2201-released</id><updated>2023-07-18T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 21.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES ENHANCEMENTS * Revisit Pod-Template in Keycloak CR keycloak operator * Support configurable custom Identity Providers keycloak * [REG 21-&gt;22] Error messages on kc build keycloak dist/quarkus BUGS * Accessibility/Clients List: Minor Issues keycloak admin/ui * `keycloakCRName` and `realm` are no longer marked as required in KeycloakRealmImport CRD keycloak operator * Version 22.0.0 not started in dev mode and build mode keycloak dist/quarkus * Migration for 22.0.0 is missing from the documentation keycloak docs * Broken links to quickstarts in documentation keycloak docs * Account V3 Missing translate Refresh keycloak account/ui * Keycloak is storing error events even if storing events is disabled keycloak storage * Fixing broken JSON translation files keycloak admin/ui UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title type="html">How to run standalone Jakarta Batch Jobs</title><link rel="alternate" href="https://www.mastertheboss.com/java-ee/batch-api/running-batch-jobs-in-j2se-applications/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java-ee/batch-api/running-batch-jobs-in-j2se-applications/</id><updated>2023-07-14T10:16:48Z</updated><content type="html">Jakarta Batch, formerly known as Java Batch, is a specification that provides a standardized approach for implementing batch processing in Java applications. It offers a robust and scalable framework for executing large-scale, long-running, and data-intensive tasks. In this tutorial, we will explore the process of running Jakarta Batch Jobs as standalone Java applications, discussing the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>A developer’s path to success with OpenShift and containers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/13/developers-path-success-openshift-and-containers" /><author><name>Valentina Rodriguez Sosa</name></author><id>e01ea10c-6333-49c0-8681-48b4641c4ebe</id><updated>2023-07-13T07:00:00Z</updated><published>2023-07-13T07:00:00Z</published><summary type="html">&lt;p&gt;I am a developer new to containers, Kubernetes, or CI/CD. Where should I start?&lt;/p&gt; &lt;p&gt;This article provides five pathways including resources to succeed on your container journey.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlighted material:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The following materials are free with no prerequisites.&lt;/li&gt; &lt;li aria-level="1"&gt;These materials are foundational for you to start working on your next project ASAP.&lt;/li&gt; &lt;li aria-level="1"&gt;Training materials will take up to five hours to complete.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;1. Start building your skills with containers and OpenShift&lt;/h2&gt; &lt;p&gt;To start with containers, understand what containers are and how CI/CD can automate the software development lifecycle. &lt;/p&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/containers#overview"&gt;Understanding containers &lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Blog: &lt;a href="https://developers.redhat.com/blog/2020/09/03/the-present-and-future-of-ci-cd-with-gitops-on-red-hat-openshift#"&gt;The present and future of CI/CD with GitOps on Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/cloud-native-apps"&gt;Understanding Cloud Native Applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Start building your skills&lt;/h3&gt; &lt;p&gt;Gather hands-on experience with video tutorials and learning paths to practice the concepts learned from foundational to advanced on OpenShift.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Video tutorial: &lt;a href="https://developers.redhat.com/learn/openshift/foundations-openshift"&gt;Foundations of OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Video tutorial: &lt;a href="https://developers.redhat.com/learn/openshift"&gt;OpenShift and Kubernetes learning&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Try the Developer Sandbox for Red Hat OpenShift: &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Start exploring in the Developer Sandbox for free&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about Kubernetes&lt;/h3&gt; &lt;p&gt;A deep dive on Kubernetes concepts from services to containers and pods: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/04/05/kubernetes-patterns-path-cloud-native"&gt;Kubernetes Patterns: The path to cloud native&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Our product documentation&lt;/h3&gt; &lt;p&gt;Discover all the features and capabilities of OpenShift from our product documentation.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/openshift_images/index.html"&gt;Overview of Images in Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/applications/index.html"&gt;Building Applications with Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/web_console/web-console-overview.html"&gt;OpenShift Web Console Overview&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;2. Modernize your applications&lt;/h2&gt; &lt;p&gt;Explore the practices to move your application to containers.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Video tutorial and additional materials: &lt;a href="https://developers.redhat.com/topics/microservices"&gt;Developing microservices on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/application-modernization/what-is-dotnet-modernization#:~:text=The%20purpose%20of%20workload%20modernization,and%20integrating%20old%20with%20new."&gt;What is .NET application modernization?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap"&gt;OpenShift QuickStarts to deploy JBossEAP&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to practice?&lt;/h3&gt; &lt;p&gt;Practice the concepts learn with our Developer Sandbox for Red Hat OpenShift, tutorials, and hands-on labs.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Tutorials: &lt;a href="https://developers.redhat.com/topics"&gt;All Development topics with Red Hat Developer &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Video tutorial: &lt;a href="https://www.redhat.com/en/services/training/do092-developing-cloud-native-applications-microservices-architectures"&gt;Developing cloud-native applications with microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hands-on lab: &lt;a href="https://developers.redhat.com/learn/openshift/develop-on-openshift"&gt;Developing on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about application development&lt;/h3&gt; &lt;p&gt;Learn about Red Hat Enterprise Linux capabilities to improve the developer experience and container applications development experience.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/05/10/how-new-rhel-92-improves-developer-experience#"&gt;How the new RHEL 9.2 improves the developer experience &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/12/12/kubernetes-native-inner-loop-development-quarkus"&gt;Kubernetes-native inner loop development with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;3. Migrate at scale with OpenShift&lt;/h2&gt; &lt;p&gt;After migrating a couple of applications, you might wonder how we can replicate this process across an organization. Discover where to start with the modernization journey and how the developer experience can be improved.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Demo video: &lt;a href="https://www.youtube.com/watch?v=Pe0bFA4WawQ"&gt;Build, test, tune, and deploy your application with Red Hat OpenShift Dev Spaces&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/05/23/podman-desktop-now-generally-available"&gt;Podman Desktop 1.0: Local container development made easy &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://docs.openshift.com/container-platform/4.13/applications/odc-viewing-application-composition-using-topology-view.html"&gt;Viewing application composition using the Topology view &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/application-modernization"&gt;Modernizing existing applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to try?&lt;/h3&gt; &lt;p&gt;Start analyzing and assessing applications with MTA. Learn from our demo and product documentation.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=u9N-T-uD_KU"&gt;Migration Toolkit For Applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about migration&lt;/h3&gt; &lt;p&gt;Plan your Java application modernization journey with our e-book and learn Podman's capabilities.&lt;/p&gt; &lt;ul&gt;&lt;li class="Indent1"&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts#"&gt;Podman basics &lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;E-book: &lt;a href="https://www.redhat.com/en/engage/java-application-modernization-20220926"&gt;A practical guide to kick-start your own initiative&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;4. Automate to accelerate your software development lifecycle&lt;/h2&gt; &lt;p&gt;Automate software development process adopting GitOps approach and secure with DevSecOps.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/09/07/how-set-your-gitops-directory-structure"&gt;How to set up your GitOps directory structure&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments"&gt;Git best practices: Workflows for GitOps deployments&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/devops/what-is-devsecops"&gt;What's DevSecOps &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation and demos: &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps: Automating security in the development lifecycle&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to try automation?&lt;/h3&gt; &lt;p&gt;Learn from these free hands-on labs how to bring automation with CI/CD and GitOps practices by using Helm, OpenShift Pipelines, Jenkins, Ansible Automation Platform, and OpenShift GitOps.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/learn/openshift/develop-gitops"&gt;Develop with GitOps &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/gitops/getting-started-openshift-pipelines"&gt;Getting Started with OpenShift Pipelines&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/cicd-ansible-automation-platform-and-jenkins-openshift"&gt;CI/CD with the Ansible Automation Platform and Jenkins on OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/gitops/working-helm"&gt;Working with Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about DevOps&lt;/h3&gt; &lt;p&gt;These e-books will help you start with best practices and practical guides to transform into a DevOps culture.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;The Path to GitOps &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/engage/devops-culture-practice-openshift-ebooks"&gt;DevOps Culture and Practice with OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Our product documentation&lt;/h3&gt; &lt;p&gt;Review our product documentation to learn about features and much more.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/cicd/index.html"&gt;OpenShift CI/CD &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/applications/working_with_helm_charts/understanding-helm.html"&gt;Understanding Helm &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/cicd/gitops/understanding-openshift-gitops.html"&gt;Understanding OpenShift GitOps&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;5. How to innovate with OpenShift&lt;/h2&gt; &lt;p&gt;Learn about key OpenShift capabilities to bring innovation to applications from serverless architectures, interconnecting services in diverse platforms, and securing and observing microservices with OpenShift Service Mesh.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/serverless"&gt;What's Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/serverless"&gt;Serverless&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://developers.redhat.com/products/service-interconnect/overview"&gt;Interconnect applications and microservices across the open hybrid cloud&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Tutorials, books, videos and more: &lt;a href="https://developers.redhat.com/topics/serverless-architecture#assembly-field-sections-38375"&gt;Build serverless architectures for Kubernetes with Knative &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/what-is-openshift-service-mesh"&gt;What's Red Hat OpenShift Service Mesh&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to try OpenShift components?&lt;/h3&gt; &lt;p&gt;Gather hands-on experience with our free labs and follow tutorials and demos at your own pace.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Hands-on lab: &lt;a href="https://developers.redhat.com/courses/getting-started-openshift-serverless"&gt;Getting Started with OpenShift Serverless &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://www.youtube.com/watch?v=YoGR5zZGG9k"&gt;OpenShift Service Mesh&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about OpenShift Service Mesh&lt;/h3&gt; &lt;p&gt;This e-book provides guidance on governance, design practices, and configuring Red Hat OpenShift Service Mesh for production use and performing day-2 operations. &lt;/p&gt; &lt;ul&gt;&lt;li&gt;E-book: &lt;a href="https://www.redhat.com/en/resources/getting-started-with-openshift-service-mesh-ebook"&gt;Getting Started with Red Hat OpenShift Service Mesh&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Find more resources in our product documentation&lt;/h3&gt; &lt;p&gt;Learn about product capabilities, features, and much more from our product documentation.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/service_mesh/v2x/ossm-about.html"&gt;OpenShift Service Mesh &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/serverless/about/about-serverless.html"&gt;OpenShift Serverless &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/distr_tracing/distributed-tracing-release-notes.html"&gt;Distributed Tracing&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/13/developers-path-success-openshift-and-containers" title="A developer’s path to success with OpenShift and containers"&gt;A developer’s path to success with OpenShift and containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Valentina Rodriguez Sosa</dc:creator><dc:date>2023-07-13T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus Newsletter #34 - July</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-newsletter-34/" /><author><name>James Cobb</name></author><id>https://quarkus.io/blog/quarkus-newsletter-34/</id><updated>2023-07-13T00:00:00Z</updated><content type="html">Read "Quarkus 3.2.0.Final released - New security features, @QuarkusComponentTest" by Guillaume Smet" to learn about major changes like; various new security features, the ability to test CDI components with @QuarkusComponentTest and new build time analytics. Kevin Dubois' article "Managing Java containers with Quarkus and Podman Desktop" shows how to build...</content><dc:creator>James Cobb</dc:creator></entry><entry><title>How to create an instance on GCP using the Ansible CLI</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/12/how-create-instance-gcp-using-ansible-cli" /><author><name>Deepankar Jain</name></author><id>db5556aa-b189-409a-9e92-a77ccdafba55</id><updated>2023-07-12T07:00:00Z</updated><published>2023-07-12T07:00:00Z</published><summary type="html">&lt;p&gt;This series covers the end-to-end process of creating an instance on Google Cloud Platform (GCP) using Red Hat Ansible Automation Platform. This 3-part series includes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 1: How to create an instance on GCP using Ansible CLI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-gcp-instance-using-ansible-automation"&gt;How to create a GCP instance using Ansible Automation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-gcp-instance-workflow-and-ansible"&gt;How to create a GCP instance via workflow and Ansible&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By the end of this article, you will have a clear understanding of how to use the Ansible Automation Platform CLI to automate the creation of GCP instances, which will save you time and reduce the risk of manual errors. Let's get started!&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Ansible&lt;/a&gt; installed on your system.&lt;/li&gt; &lt;li aria-level="1"&gt;An active GCP Account with sufficient permissions.&lt;/li&gt; &lt;li aria-level="1"&gt;Ansible &lt;a href="https://galaxy.ansible.com/google/cloud"&gt;google cloud collection&lt;/a&gt; installed on your system.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We will start by setting up the necessary credentials and roles for our Ansible playbook to access the GCP API. Then we will create a disk, a network, a security group, and an IP address before finally launching the instance.&lt;/p&gt; &lt;h2&gt;How to use Ansible CLI&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Create a &lt;a href="https://support.google.com/a/answer/7378726?hl=en"&gt;service account&lt;/a&gt; in GCP.&lt;/li&gt; &lt;li aria-level="1"&gt;Generate the &lt;a href="https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account"&gt;credentials&lt;/a&gt; for the service account.&lt;/li&gt; &lt;li aria-level="1"&gt;You should now have a credential.json&lt;strong&gt; &lt;/strong&gt;file&lt;strong&gt; &lt;/strong&gt;that you can use to access your GCP account and launch an instance.&lt;/li&gt; &lt;li aria-level="1"&gt;Open any editor and copy the following yml into it.&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Create instance in GCP   hosts: localhost   gather_facts: false   vars:     service_account_file: "&lt;path to service account file&gt;"     project: "&lt;SOMETHING&gt;"     network_name: "test-ansible-network"     subnet_name: "test-ansible-subnet"     ip_name: "test-ansible-ip"     disk_name: "test-ansible-disk"     machine_name: "test-ansible"     region: "asia-south2"     zone: "asia-south2-a"     source_image: "projects/ubuntu-os-cloud/global/images/family/ubuntu-1804-lts"     subnet_cidr: "10.0.1.0/24"     disk_size: 10     machine_type: "f1-micro"   tasks:     - name: Create a disk       google.cloud.gcp_compute_disk:         name: "{{ disk_name }}"         size_gb: "{{ disk_size }}"         source_image: "{{ source_image }}"         zone: "{{ zone }}"         project: "{{ project }}"         auth_kind: serviceaccount         service_account_file: "{{ service_account_file }}"         state: present       register: disk          - name: Create a Network in GCP       google.cloud.gcp_compute_network:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         name: "{{ network_name }}"         auto_create_subnetworks: false         state: present       register: network     - name: Create a Subnet in the Network       google.cloud.gcp_compute_subnetwork:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         name: "{{ subnet_name }}"         region: "{{ region }}"         ip_cidr_range: "{{ subnet_cidr }}"         network: "{{ network }}"         state: present       register: subnet     - name: Reserve a static IP Address       google.cloud.gcp_compute_address:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         name: "{{ ip_name }}"         region: "{{ region }}"         state: present       register: address              - name: Create an Instance        google.cloud.gcp_compute_instance:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         state: present         name: "{{ machine_name }}"         machine_type: "{{ machine_type }}"         zone: "{{ zone }}"         disks:           - auto_delete: true             boot: true             source: "{{ disk }}"         network_interfaces:           - network: "{{ network }}"             subnetwork: "{{ subnet }}"             access_configs:               - name: External NAT                 type: ONE_TO_ONE_NAT                 nat_ip: "{{ address }}" &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Save and close the file.&lt;/li&gt; &lt;li&gt;Then open the terminal in the directory where the file is located.&lt;/li&gt; &lt;li aria-level="1"&gt;Now run the following command: &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook &lt;filename&gt;.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;PLAY [Create instance in GCP] ************************************************************************************************************************************************************ TASK [Gathering Facts] ******************************************************************************************************************************************************************************************** ok: [localhost] TASK [Create a disk] ************************************************************************************************************************************************************************************ changed: [localhost] TASK [Create a Network in GCP] *********************************************************************************************************************************************************************************** changed: [localhost] TASK [Create a Subnet in the Network] ******************************************************************************************************************************************************************************************** changed: [localhost] TASK [Reserve a static IP Address] ********************************************************************************************************************************************************************************* changed: [localhost] TASK [Create an Instance] ******************************************************************************************************************************************* changed: [localhost] PLAY RECAP ******************************************************************************************************************************************************************************************************** localhost : ok=6 changed=5 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GCP instance is shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_10-54-25.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_10-54-25.png?itok=g2s4Y660" width="600" height="169" alt="Creating a GCP instance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Creating a GCP instance.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;By following the step-by-step guide, you should now have a good understanding of how to use Ansible to automate the creation of a virtual machine. To learn more about Ansible and access additional resources and guides, including diverse examples and use cases, we recommend visiting &lt;a href="https://developers.redhat.com/learn/ansible"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What’s next?&lt;/h2&gt; &lt;p&gt;In our &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-gcp-instance-using-ansible-automation"&gt;next article&lt;/a&gt;, we will explore how &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible Automation Platform&lt;/a&gt; further eases the process of creating virtual machines by enabling you to define infrastructure as code, track infrastructure changes, and enforce compliance policies. If you're interested in exploring how to use &lt;a href="https://developers.redhat.com/learn/ansible"&gt;Ansible Automation Platform&lt;/a&gt; on Azure, you can also access the &lt;a href="https://developers.redhat.com/content-gateway/link/3872066"&gt;lab&lt;/a&gt;. This lab allows you to try Ansible Automation Platform on Azure and learn how it can be used to automate infrastructure deployment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started&lt;/a&gt; with Ansible Automation Platform by exploring interactive hands-on labs. &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Download Ansible Automation Platform&lt;/a&gt; at no cost and begin your automation journey. You can refer to &lt;a href="https://developers.redhat.com/e-books/choosing-automation-tool"&gt;An IT executive's guide to automation&lt;/a&gt; e-book for a better understanding of the Ansible Automation Platform.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-instance-gcp-using-ansible-cli" title="How to create an instance on GCP using the Ansible CLI"&gt;How to create an instance on GCP using the Ansible CLI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain</dc:creator><dc:date>2023-07-12T07:00:00Z</dc:date></entry><entry><title type="html">On the Road to CDI Compatibility</title><link rel="alternate" href="https://quarkus.io/blog/on-the-road-to-cdi-compatibility/" /><author><name>Ladislav Thon</name></author><id>https://quarkus.io/blog/on-the-road-to-cdi-compatibility/</id><updated>2023-07-12T00:00:00Z</updated><content type="html">Ever since the very first days of Quarkus, the days that are now covered by the blissful fog of oblivion and the survivors only talk about them after a few pints of beer, dependency injection container was an integral part of the envisioned framework. And not just any dependency injection...</content><dc:creator>Ladislav Thon</dc:creator></entry><entry><title type="html">Groupby &amp;#8211; a new way to accumulate facts in DRL</title><link rel="alternate" href="https://blog.kie.org/2023/07/groupby-a-new-way-to-accumulate-facts-in-drl.html" /><author><name>Christopher Chianelli</name></author><id>https://blog.kie.org/2023/07/groupby-a-new-way-to-accumulate-facts-in-drl.html</id><updated>2023-07-11T09:00:00Z</updated><content type="html">Have you ever wanted to accumulate facts that share a particular property (for instance, the count of people in each department) in DRL? With the addition of groupby to DRL, it is easier to write rules that divide facts into groups where each group is accumulated separately. In this post, we’ll explain what groupby is, how to use it, and give examples of groupby usage. WHAT IS GROUPBY? Groupby is a new syntax construct introduced in 8.41.0.Final for dividing facts into groups, allowing each group to be accumulated separately. It has the following syntax: groupby(source-pattern; grouping-key; accumulators [; constraints]) Where: * source-pattern: Pattern used to gather facts that will be grouped and accumulated. For instance, using $a: /applicants[age &lt; 21] as the source-pattern would group and accumulate all Applicants under the age of 21 (binding the applicant to $a, which can be used in the grouping-key and accumulators). The examples in this article use the OOPath syntax for source-pattern, but the traditional syntax is also supported. For more details, see . * grouping-key: A function used to divide the source-pattern into separate groups. Facts from source-pattern for which grouping-key returns the same value are grouped together. The key can optionally be bound to a variable, allowing it to be used outside the groupby. For example, using $country: $a.country as the grouping key would group applicants by country, binding the country for the group to the $country variable. * accumulators: One or more accumulate functions used to accumulate the facts in each group. Each accumulate function can optionally be bound to a variable, allowing it to be used outside the groupby. For instance, $count: count() would count the number of applicants younger than 21 from each country. * constraints: Optional constraints on the group key and accumulation result. The rest of the rule will only execute for the given group key if all constraints return true. For instance, $count &gt;= 100 would only continue execution of the rule for a given $country if that country have at least 100 applicants younger than 21. Combining the above example together, we get: groupby( $a: /applicants[age &lt; 21]; $country: $a.country; $count: count(); $count &gt;= 100 ) HOW TO USE GROUPBY You can use groupby by upgrading Drools to 8.41.0.Final or later. Groupby can then be used in your DRL files like any other language construct (such as forall and accumulate). EXAMPLE GROUPBY USAGE Groupby is a good choice whenever you need to accumulate results for separate groups. Some examples of rules that can be implemented with it include: * Get departments over budget rule "Departments over budget" groupby($order: /order; $department: $order.department; $totalCost: sum($order.cost); $totalCost &gt; $department.budget ) then // ... end * Find days which are understaffed rule "Understaffed Days" groupby($shift: /shifts[ employee != null ]; $date: $shift.date; $assignedCount: count(); $assignedCount &lt; $date.minimumAssignedShifts ) then // ... end * Get the highest bid for a product rule "Highest bid for a product" groupby($bid: /bids; $product: $bid.product; $highestBid: max($bid) ) then // ... end CONCLUSION Groupby is a new language feature introduced in 8.41.0.Final that allows for simpler grouping of objects by a key function. You can use it by upgrading Drools to 8.41.0.Final or later. Groupby is useful for implementing rules that accumulate facts that share a given property, such as getting the total cost by department or getting the highest bidder by product. The post appeared first on .</content><dc:creator>Christopher Chianelli</dc:creator></entry></feed>
